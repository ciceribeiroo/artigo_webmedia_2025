{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install scikeras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-26 23:36:00.826669: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory\n",
      "2025-05-26 23:36:00.826689: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n",
      "[nltk_data] Downloading package stopwords to /home/alice/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "from numpy.random import seed\n",
    "seed(1)\n",
    "import tensorflow \n",
    "tensorflow.random.set_seed(1) \n",
    "import pandas as pd\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "stopwords = nltk.corpus.stopwords.words('portuguese')\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from collections import Counter\n",
    "from tensorflow import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers.core import Dense, Dropout, Flatten, Activation\n",
    "from keras.layers.convolutional import Conv1D, MaxPooling1D, Convolution1D\n",
    "from keras.optimizers import Adadelta\n",
    "from sklearn import metrics\n",
    "from scikeras.wrappers import KerasClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import train_test_split\n",
    "import json\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_stopwords(text):\n",
    "    all_words = text.split(\" \")\n",
    "    clean_text = [i for i in all_words if i not in stopwords and i!=\"\"]\n",
    "    return \" \".join(clean_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_html_tags(text):\n",
    "    import re\n",
    "    clean = re.compile('<.*?>')\n",
    "    return re.sub(clean, '', text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lower_texts(text):\n",
    "    return text.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(text):\n",
    "    clean_text = remove_stopwords(text)\n",
    "    clean_text = remove_html_tags(clean_text)\n",
    "    clean_text = lower_texts(clean_text)\n",
    "    return clean_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model_age(filters = [100], kernel_size = [50], strides = [100], \n",
    "                 dropout_rate = 0.5, pool_size = [5], dense_units = 100, max_len = 1000):\n",
    "\n",
    "    model = Sequential()\n",
    "\n",
    "    # conv 1\n",
    "    model.add(Conv1D(filters = filters[0], \n",
    "                     kernel_size = kernel_size[0],\n",
    "                     strides = strides[0], \n",
    "                     activation = 'relu', \n",
    "                     input_shape = (max_len, 1) ))\n",
    "\n",
    "    # pooling layer 1\n",
    "    for i in range(len(pool_size)):\n",
    "        model.add(MaxPooling1D(pool_size = pool_size[i], strides = 1))\n",
    "        model.add(Activation('relu'))\n",
    "    \n",
    "    #model.add(Activation('relu'))\n",
    "    \n",
    "    model.add(Flatten())\n",
    "    \n",
    "    if dropout_rate is not None:\n",
    "        model.add(Dropout(dropout_rate))\n",
    "        \n",
    "    model.add(Dense(units = dense_units, activation = 'relu'))\n",
    "    model.add(Dense(units = 3, activation = 'softmax'))\n",
    "\n",
    "    model.compile(loss='categorical_crossentropy', optimizer = Adadelta(\n",
    "       learning_rate=1, name=\"Adadelta\"\n",
    "    ), metrics = ['accuracy'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "\n",
    "def formatTime(seg):\n",
    "    return str(datetime.timedelta(seconds=seg))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"../data/blogset-without-duplicate.csv\")\n",
    "\n",
    "y = df[\"Age\"]\n",
    "\n",
    "df_train, df_test = train_test_split(df, test_size=0.2, random_state=115, stratify=y)\n",
    "\n",
    "# df_train = pd.read_csv('../data/train_1.csv')\n",
    "# df_test = pd.read_csv('../data/test_1.csv')\n",
    "\n",
    "X_train = df_train[\"Texts\"].apply(clean_text).to_numpy()\n",
    "X_test = df_test[\"Texts\"].apply(clean_text).to_numpy()\n",
    "y_train = df_train[\"Age\"].to_numpy()\n",
    "y_test = df_test[\"Age\"].to_numpy()\n",
    "\n",
    "num_words = []\n",
    "for t in X_train:\n",
    "    num_words.append(len(t.split()))\n",
    "\n",
    "mean = sum(num_words)//len(num_words)\n",
    "\n",
    "train_texts = X_train.tolist()\n",
    "test_texts = X_test.tolist()\n",
    "tfidfvec = TfidfVectorizer(max_features = mean, max_df=0.9)\n",
    "tfidfvec.fit(train_texts)\n",
    "tfidf_train = tfidfvec.transform(train_texts).toarray()\n",
    "tfidf_test = tfidfvec.transform(test_texts).toarray()\n",
    "\n",
    "X_train = tfidf_train.reshape(tfidf_train.shape[0],tfidf_train.shape[1],1)\n",
    "X_test = tfidf_test.reshape(tfidf_test.shape[0],tfidf_test.shape[1],1)\n",
    "y_train = keras.utils.to_categorical(y_train,num_classes=3)\n",
    "y_test = keras.utils.to_categorical(y_test,num_classes=3)\n",
    "\n",
    "size = X_test.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.callbacks import EarlyStopping \n",
    "\n",
    "callback = [\n",
    "        EarlyStopping(patience=5, monitor='val_accuracy', mode='max', restore_best_weights=True),\n",
    "    ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3769"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model_age1(filters = [100], kernel_size = [50], strides = [100], \n",
    "                 dropout_rate = 0.5, pool_size = [5], dense_units = 100, max_len = 1000):\n",
    "\n",
    "    model = Sequential()\n",
    "\n",
    "    # conv 1\n",
    "    model.add(Conv1D(filters = filters[0], \n",
    "                     kernel_size = kernel_size[0],\n",
    "                     strides = strides[0], \n",
    "                     activation = 'relu', \n",
    "                     input_shape = (max_len, 1) ))\n",
    "\n",
    "    # pooling layer 1\n",
    "    for i in range(len(pool_size)):\n",
    "        model.add(MaxPooling1D(pool_size = pool_size[i], strides = 1))\n",
    "        model.add(Activation('relu'))\n",
    "    \n",
    "    model.add(Activation('relu'))\n",
    "    \n",
    "    model.add(Flatten())\n",
    "    \n",
    "    if dropout_rate is not None:\n",
    "        model.add(Dropout(dropout_rate))\n",
    "        \n",
    "    model.add(Dense(units = dense_units, activation = 'relu'))\n",
    "    model.add(Dense(units = 3, activation = 'softmax'))\n",
    "\n",
    "    model.compile(loss='categorical_crossentropy', optimizer = Adadelta(\n",
    "       learning_rate=1, name=\"Adadelta\"\n",
    "    ), metrics = ['accuracy'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(508, 3769, 1)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-26 23:39:03.332244: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:975] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2025-05-26 23:39:03.333000: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory\n",
      "2025-05-26 23:39:03.333455: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcublas.so.11'; dlerror: libcublas.so.11: cannot open shared object file: No such file or directory\n",
      "2025-05-26 23:39:03.333518: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcublasLt.so.11'; dlerror: libcublasLt.so.11: cannot open shared object file: No such file or directory\n",
      "2025-05-26 23:39:03.333728: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcufft.so.10'; dlerror: libcufft.so.10: cannot open shared object file: No such file or directory\n",
      "2025-05-26 23:39:03.333784: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcurand.so.10'; dlerror: libcurand.so.10: cannot open shared object file: No such file or directory\n",
      "2025-05-26 23:39:03.333836: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcusolver.so.11'; dlerror: libcusolver.so.11: cannot open shared object file: No such file or directory\n",
      "2025-05-26 23:39:03.333884: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcusparse.so.11'; dlerror: libcusparse.so.11: cannot open shared object file: No such file or directory\n",
      "2025-05-26 23:39:03.334085: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudnn.so.8'; dlerror: libcudnn.so.8: cannot open shared object file: No such file or directory\n",
      "2025-05-26 23:39:03.334094: W tensorflow/core/common_runtime/gpu/gpu_device.cc:1850] Cannot dlopen some GPU libraries. Please make sure the missing libraries mentioned above are installed properly if you would like to use GPU. Follow the guide at https://www.tensorflow.org/install/gpu for how to download and setup the required libraries for your platform.\n",
      "Skipping registering GPU devices...\n",
      "2025-05-26 23:39:03.334306: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2025-05-26 23:39:03.366410: W tensorflow/core/framework/cpu_allocator_impl.cc:82] Allocation of 462766080 exceeds 10% of free system memory.\n",
      "2025-05-26 23:39:03.462956: W tensorflow/core/framework/cpu_allocator_impl.cc:82] Allocation of 462766080 exceeds 10% of free system memory.\n",
      "2025-05-26 23:39:03.645427: W tensorflow/core/framework/cpu_allocator_impl.cc:82] Allocation of 462766080 exceeds 10% of free system memory.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-26 23:39:04.073273: W tensorflow/core/framework/cpu_allocator_impl.cc:82] Allocation of 462766080 exceeds 10% of free system memory.\n",
      "2025-05-26 23:39:04.100538: W tensorflow/core/framework/cpu_allocator_impl.cc:82] Allocation of 462766080 exceeds 10% of free system memory.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "64/64 [==============================] - 35s 532ms/step - loss: 1.0615 - accuracy: 0.4293 - val_loss: 1.0262 - val_accuracy: 0.4724\n",
      "Epoch 2/10\n",
      "64/64 [==============================] - 33s 520ms/step - loss: 0.9792 - accuracy: 0.5057 - val_loss: 0.9891 - val_accuracy: 0.4744\n",
      "Epoch 3/10\n",
      "64/64 [==============================] - 33s 516ms/step - loss: 0.8982 - accuracy: 0.5692 - val_loss: 1.0031 - val_accuracy: 0.4843\n",
      "Epoch 4/10\n",
      "64/64 [==============================] - 32s 503ms/step - loss: 0.8127 - accuracy: 0.6259 - val_loss: 1.1395 - val_accuracy: 0.4528\n",
      "Epoch 5/10\n",
      "64/64 [==============================] - 32s 503ms/step - loss: 0.7532 - accuracy: 0.6506 - val_loss: 1.2890 - val_accuracy: 0.4370\n",
      "Epoch 6/10\n",
      "64/64 [==============================] - 32s 502ms/step - loss: 0.6762 - accuracy: 0.7013 - val_loss: 1.2491 - val_accuracy: 0.4488\n",
      "Epoch 7/10\n",
      "64/64 [==============================] - 32s 505ms/step - loss: 0.6134 - accuracy: 0.7260 - val_loss: 1.2601 - val_accuracy: 0.4508\n",
      "Epoch 8/10\n",
      "64/64 [==============================] - 32s 505ms/step - loss: 0.5309 - accuracy: 0.7684 - val_loss: 1.4441 - val_accuracy: 0.4311\n",
      "16/16 [==============================] - 1s 65ms/step\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'np' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m/home/alice/Downloads/blogsetBr (1)/baseline/baseline copy 2.ipynb Cell 14\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/alice/Downloads/blogsetBr%20%281%29/baseline/baseline%20copy%202.ipynb#X21sZmlsZQ%3D%3D?line=18'>19</a>\u001b[0m model_age\u001b[39m.\u001b[39mfit(X_train,y_train_age,validation_data\u001b[39m=\u001b[39m(X_test,y_test_age_cat), \n\u001b[1;32m     <a href='vscode-notebook-cell:/home/alice/Downloads/blogsetBr%20%281%29/baseline/baseline%20copy%202.ipynb#X21sZmlsZQ%3D%3D?line=19'>20</a>\u001b[0m                 batch_size\u001b[39m=\u001b[39m\u001b[39m32\u001b[39m,\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/alice/Downloads/blogsetBr%20%281%29/baseline/baseline%20copy%202.ipynb#X21sZmlsZQ%3D%3D?line=20'>21</a>\u001b[0m                 epochs\u001b[39m=\u001b[39m\u001b[39m10\u001b[39m,\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/alice/Downloads/blogsetBr%20%281%29/baseline/baseline%20copy%202.ipynb#X21sZmlsZQ%3D%3D?line=21'>22</a>\u001b[0m                 callbacks\u001b[39m=\u001b[39mcallback)\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/alice/Downloads/blogsetBr%20%281%29/baseline/baseline%20copy%202.ipynb#X21sZmlsZQ%3D%3D?line=23'>24</a>\u001b[0m y_pred_age \u001b[39m=\u001b[39m model_age\u001b[39m.\u001b[39mpredict(\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/alice/Downloads/blogsetBr%20%281%29/baseline/baseline%20copy%202.ipynb#X21sZmlsZQ%3D%3D?line=24'>25</a>\u001b[0m     X_test\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/alice/Downloads/blogsetBr%20%281%29/baseline/baseline%20copy%202.ipynb#X21sZmlsZQ%3D%3D?line=25'>26</a>\u001b[0m )\n\u001b[0;32m---> <a href='vscode-notebook-cell:/home/alice/Downloads/blogsetBr%20%281%29/baseline/baseline%20copy%202.ipynb#X21sZmlsZQ%3D%3D?line=27'>28</a>\u001b[0m y_pred_list_age \u001b[39m=\u001b[39m [np\u001b[39m.\u001b[39margmax(x, axis\u001b[39m=\u001b[39m\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m) \u001b[39mfor\u001b[39;00m x \u001b[39min\u001b[39;00m y_pred_age]\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/alice/Downloads/blogsetBr%20%281%29/baseline/baseline%20copy%202.ipynb#X21sZmlsZQ%3D%3D?line=29'>30</a>\u001b[0m \u001b[39mprint\u001b[39m(metrics\u001b[39m.\u001b[39mf1_score(y_test_age, y_pred_list_age, average\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mmacro\u001b[39m\u001b[39m'\u001b[39m))\n",
      "\u001b[1;32m/home/alice/Downloads/blogsetBr (1)/baseline/baseline copy 2.ipynb Cell 14\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/alice/Downloads/blogsetBr%20%281%29/baseline/baseline%20copy%202.ipynb#X21sZmlsZQ%3D%3D?line=18'>19</a>\u001b[0m model_age\u001b[39m.\u001b[39mfit(X_train,y_train_age,validation_data\u001b[39m=\u001b[39m(X_test,y_test_age_cat), \n\u001b[1;32m     <a href='vscode-notebook-cell:/home/alice/Downloads/blogsetBr%20%281%29/baseline/baseline%20copy%202.ipynb#X21sZmlsZQ%3D%3D?line=19'>20</a>\u001b[0m                 batch_size\u001b[39m=\u001b[39m\u001b[39m32\u001b[39m,\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/alice/Downloads/blogsetBr%20%281%29/baseline/baseline%20copy%202.ipynb#X21sZmlsZQ%3D%3D?line=20'>21</a>\u001b[0m                 epochs\u001b[39m=\u001b[39m\u001b[39m10\u001b[39m,\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/alice/Downloads/blogsetBr%20%281%29/baseline/baseline%20copy%202.ipynb#X21sZmlsZQ%3D%3D?line=21'>22</a>\u001b[0m                 callbacks\u001b[39m=\u001b[39mcallback)\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/alice/Downloads/blogsetBr%20%281%29/baseline/baseline%20copy%202.ipynb#X21sZmlsZQ%3D%3D?line=23'>24</a>\u001b[0m y_pred_age \u001b[39m=\u001b[39m model_age\u001b[39m.\u001b[39mpredict(\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/alice/Downloads/blogsetBr%20%281%29/baseline/baseline%20copy%202.ipynb#X21sZmlsZQ%3D%3D?line=24'>25</a>\u001b[0m     X_test\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/alice/Downloads/blogsetBr%20%281%29/baseline/baseline%20copy%202.ipynb#X21sZmlsZQ%3D%3D?line=25'>26</a>\u001b[0m )\n\u001b[0;32m---> <a href='vscode-notebook-cell:/home/alice/Downloads/blogsetBr%20%281%29/baseline/baseline%20copy%202.ipynb#X21sZmlsZQ%3D%3D?line=27'>28</a>\u001b[0m y_pred_list_age \u001b[39m=\u001b[39m [np\u001b[39m.\u001b[39margmax(x, axis\u001b[39m=\u001b[39m\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m) \u001b[39mfor\u001b[39;00m x \u001b[39min\u001b[39;00m y_pred_age]\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/alice/Downloads/blogsetBr%20%281%29/baseline/baseline%20copy%202.ipynb#X21sZmlsZQ%3D%3D?line=29'>30</a>\u001b[0m \u001b[39mprint\u001b[39m(metrics\u001b[39m.\u001b[39mf1_score(y_test_age, y_pred_list_age, average\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mmacro\u001b[39m\u001b[39m'\u001b[39m))\n",
      "\u001b[0;31mNameError\u001b[0m: name 'np' is not defined"
     ]
    }
   ],
   "source": [
    "for i in range(1):\n",
    "    X_train = tfidf_train.reshape(tfidf_train.shape[0],tfidf_train.shape[1],1)\n",
    "    X_test = tfidf_test.reshape(tfidf_test.shape[0],tfidf_test.shape[1],1)\n",
    "\n",
    "    y_train_age = df_train[\"Age\"].to_numpy()\n",
    "    y_test_age = df_test[\"Age\"].to_numpy()\n",
    "\n",
    "    y_train_age = keras.utils.to_categorical(y_train_age,num_classes=3)\n",
    "    y_test_age_cat = keras.utils.to_categorical(y_test_age,num_classes=3)\n",
    "\n",
    "    model_age = create_model_age1([60], \n",
    "                                        kernel_size=[1], \n",
    "                                        strides=[1], \n",
    "                                        dropout_rate=0.6, \n",
    "                                        pool_size=[4], \n",
    "                                        dense_units=512, \n",
    "                                        max_len=X_test.shape[1])\n",
    "\n",
    "    model_age.fit(X_train,y_train_age,validation_data=(X_test,y_test_age_cat), \n",
    "                    batch_size=32,\n",
    "                    epochs=10,\n",
    "                    callbacks=callback)\n",
    "\n",
    "    y_pred_age = model_age.predict(\n",
    "        X_test\n",
    "    )\n",
    "\n",
    "    y_pred_list_age = [np.argmax(x, axis=-1) for x in y_pred_age]\n",
    "\n",
    "    print(metrics.f1_score(y_test_age, y_pred_list_age, average='macro'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_age.save('final_model.keras')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.48256185382192296\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "\n",
    "y_pred_list_age = [np.argmax(x, axis=-1) for x in y_pred_age]\n",
    "\n",
    "print(metrics.f1_score(y_test_age, y_pred_list_age, average='macro'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test['Predict']=y_pred_list_age"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.48256185382192296"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metrics.f1_score(y_test_age, y_pred_list_age, average='macro')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DICT F1:0.49968992248062016)\n",
      "TRAD F1: 0.5167515341428385\n"
     ]
    }
   ],
   "source": [
    "dict_class_2 = pd.read_csv('../data/dict_class_2.csv', encoding='latin-1')\n",
    "\n",
    "df_test = df_test.reset_index(drop=True)\n",
    "dict_class_2 = dict_class_2.reset_index(drop=True)\n",
    "\n",
    "df_test['AuthorID'] = df_test['AuthorID'].astype(int)\n",
    "dict_class_2['AuthorID'] = dict_class_2['AuthorID'].astype(int)\n",
    "\n",
    "compare_test_2 = df_test[df_test['AuthorID'].isin(dict_class_2['AuthorID'])]\n",
    "\n",
    "compare_test_2 = compare_test_2[['AuthorID', 'Age', 'Texts', f'Predict']]\n",
    "compare_dict_2 = dict_class_2[['AuthorID', 'c']]\n",
    "\n",
    "compare_2 = compare_test_2.merge(compare_dict_2, left_on='AuthorID', right_on='AuthorID')\n",
    "\n",
    "print(f\"DICT F1:{metrics.f1_score(compare_2['Age'], compare_2['c'], average='macro')})\")\n",
    "print(f\"TRAD F1: {metrics.f1_score(compare_2['Age'], compare_2[f'Predict'], average='macro')}\")\n",
    "#compare_2.shape, metrics.f1_score(compare_2['Age'], compare_2[f'Predict'], average='macro'), metrics.f1_score(compare_2['Age'], compare_2['c'], average='macro')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DIC ACC:0.5421686746987951)\n",
      "DICT ACC: 0.5180722891566265\n"
     ]
    }
   ],
   "source": [
    "print(f\"DIC ACC:{metrics.accuracy_score(compare_2['Age'], compare_2['c'])})\")\n",
    "print(f\"DICT ACC: {metrics.accuracy_score(compare_2['Age'], compare_2[f'Predict'])}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DICT F1:0.49086251439192624)\n",
      "TRAD F1: 0.48256185382192296\n"
     ]
    }
   ],
   "source": [
    "merged_df_2 = df_test.merge(compare_2[['AuthorID', 'c']], on='AuthorID', how='left')\n",
    "merged_df_2[f'Predict'] = merged_df_2['c'].combine_first(merged_df_2[f'Predict'])\n",
    "\n",
    "print(f\"DICT F1:{metrics.f1_score(merged_df_2['Age'], merged_df_2[f'Predict'], average='macro')})\")\n",
    "print(f\"TRAD F1: {metrics.f1_score(df_test['Age'], df_test[f'Predict'], average='macro')}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DICT F1:0.4881889763779528)\n",
      "TRAD F1: 0.484251968503937\n"
     ]
    }
   ],
   "source": [
    "print(f\"DICT F1:{metrics.accuracy_score(merged_df_2['Age'], merged_df_2[f'Predict'])})\")\n",
    "print(f\"TRAD F1: {metrics.accuracy_score(df_test['Age'], df_test[f'Predict'])}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8333333333333334"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "compare_2.shape[1]/df_test.shape[1]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.6 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "ac26e9ee9434047e2b6179ca7c094dd6d7a5c4307efa62cbdc343ee78cec5e23"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
