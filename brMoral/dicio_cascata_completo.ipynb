{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "AChuxBSUZgMk",
    "outputId": "8ab16625-3d8c-40c8-cc13-e3f932d74903"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\alici\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "import string\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "stopwords = nltk.corpus.stopwords.words('portuguese')\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "import json\n",
    "import time\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn import metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "dD-6oO3-hxyd"
   },
   "outputs": [],
   "source": [
    "pd.options.mode.chained_assignment = None  # default='warn'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "7LMU6BhwZ3u2"
   },
   "outputs": [],
   "source": [
    "file_path = r\"C:\\Users\\alici\\Documents\\UFLA\\mestrado\\projeto\\dicionario\\2024\\brMoral\\brMoral.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "et_ExMA5Zxbk"
   },
   "outputs": [],
   "source": [
    "def read_file_as_df(file_name):\n",
    "    import pandas as pd\n",
    "    import csv\n",
    "\n",
    "    import sys\n",
    "    import pandas as pd\n",
    "\n",
    "    maxInt = sys.maxsize\n",
    "\n",
    "    while True:\n",
    "        # decrease the maxInt value by factor 10\n",
    "        # as long as the OverflowError occurs.\n",
    "\n",
    "        try:\n",
    "            csv.field_size_limit(maxInt)\n",
    "            break\n",
    "        except OverflowError:\n",
    "            maxInt = int(maxInt/10)\n",
    "\n",
    "    file = []\n",
    "    col = []\n",
    "\n",
    "    with open(file_name, encoding='latin-1') as csv_file:\n",
    "        csv_reader = csv.reader(csv_file, delimiter=';')\n",
    "        line_count = 0\n",
    "        for row in csv_reader:\n",
    "            if line_count==0:\n",
    "                for r in row:\n",
    "                    col.append(r)\n",
    "                line_count+=1\n",
    "            else:\n",
    "                line = []\n",
    "                for r in row:\n",
    "                    line.append(r)\n",
    "                file.append(line)\n",
    "                line_count += 1\n",
    "\n",
    "\n",
    "    df = pd.DataFrame(file, columns = col)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "BpizUBqWalJb"
   },
   "outputs": [],
   "source": [
    "def remove_stopwords(text):\n",
    "    all_words = text.split(\" \")\n",
    "    clean_text = [i for i in all_words if i not in stopwords and i!=\"\"]\n",
    "    return \" \".join(clean_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "hvc8Ibrsams6"
   },
   "outputs": [],
   "source": [
    "def clean_text(x):\n",
    "  return \" \".join(str(x.translate(str.maketrans('', '', string.punctuation))).split()).lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "YBstiYpNcRz5"
   },
   "outputs": [],
   "source": [
    "def select_abs(col, perc):\n",
    "  n = int(len(df_coef) * perc)\n",
    "\n",
    "  df_coef_1 = df_coef.copy()\n",
    "  df_coef_1 = df_coef_1[[col]]\n",
    "\n",
    "  df_coef_1['abs_values'] = df_coef_1[col].abs()\n",
    "\n",
    "  sorted_df = df_coef_1.sort_values(by='abs_values', ascending=False)\n",
    "\n",
    "  return sorted_df.head(n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "fold_path = r'C:\\Users\\alici\\Documents\\UFLA\\mestrado\\projeto\\dicionario\\2024\\brMoral\\folds'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(r\"C:\\Users\\alici\\Documents\\UFLA\\mestrado\\projeto\\dicionario\\2024\\brMoral\\age.json\", encoding='utf-8') as f:\n",
    "    age_param = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'grid_values': {'C': [0.001, 0.01, 0.1, 1.0, 10.0, 100.0, 1000.0, 2857.14],\n",
       "  'penalty': ['l1', 'l2']},\n",
       " 'k_1': {'params': {'C': 10.0, 'penalty': 'l1'}, 'f1': 0.44333333333333336},\n",
       " 'k_2': {'params': {'C': 1.0, 'penalty': 'l2'}, 'f1': 0.4700000000000001},\n",
       " 'k_3': {'params': {'C': 1.0, 'penalty': 'l2'}, 'f1': 0.6066666666666666},\n",
       " 'k_4': {'params': {'C': 10.0, 'penalty': 'l2'}, 'f1': 0.4833333333333334},\n",
       " 'k_5': {'params': {'C': 1000.0, 'penalty': 'l1'}, 'f1': 0.5466666666666666},\n",
       " 'k_6': {'params': {'C': 0.001, 'penalty': 'l1'}, 'f1': 0.3733333333333334},\n",
       " 'k_7': {'params': {'C': 1.0, 'penalty': 'l2'}, 'f1': 0.4566666666666667},\n",
       " 'k_8': {'params': {'C': 0.001, 'penalty': 'l1'}, 'f1': 0.3533333333333334},\n",
       " 'k_9': {'params': {'C': 1000.0, 'penalty': 'l2'}, 'f1': 0.41},\n",
       " 'k_10': {'params': {'C': 100.0, 'penalty': 'l1'}, 'f1': 0.38666666666666666},\n",
       " 'time': '0.0min8.49s'}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "age_param"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df =  pd.read_csv(fold_path+r'\\k_'+str(1)+r'\\train.csv')\n",
    "test_df =  pd.read_csv(fold_path+r'\\k_'+str(1)+r'\\test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_merged = pd.concat([train_df, test_df], ignore_index=True, sort=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_merged['num_palavras'] = df_merged['concat'].apply(lambda x: len(str(x).split()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sid</th>\n",
       "      <th>concat</th>\n",
       "      <th>freetext</th>\n",
       "      <th>gender</th>\n",
       "      <th>ap.age</th>\n",
       "      <th>num_palavras</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>sid6000</td>\n",
       "      <td>Não vejo e nem nunca vi problema nisso, acho q...</td>\n",
       "      <td>A flexibilidade é extremamente importante nas ...</td>\n",
       "      <td>m</td>\n",
       "      <td>1</td>\n",
       "      <td>272</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>sid6001</td>\n",
       "      <td>Acredito que todos possuem direito de escolher...</td>\n",
       "      <td>Acabo de responder a perguntas bastante polêmi...</td>\n",
       "      <td>m</td>\n",
       "      <td>2</td>\n",
       "      <td>416</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>sid959</td>\n",
       "      <td>Acredito que todos tem o direito de escolher c...</td>\n",
       "      <td>Achei esse teste cansativo à beça, mas me fez ...</td>\n",
       "      <td>m</td>\n",
       "      <td>2</td>\n",
       "      <td>373</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>sid842</td>\n",
       "      <td>Vejo que as pessoas tem todo o direito de esco...</td>\n",
       "      <td>amanhã trabalho cansaço tarde engenharia dados...</td>\n",
       "      <td>m</td>\n",
       "      <td>1</td>\n",
       "      <td>664</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>sid774</td>\n",
       "      <td>As pessoas devem ser livres para tomar decisõe...</td>\n",
       "      <td>Algo difícil na vida é conquistar sua independ...</td>\n",
       "      <td>f</td>\n",
       "      <td>1</td>\n",
       "      <td>304</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       sid                                             concat  \\\n",
       "0  sid6000  Não vejo e nem nunca vi problema nisso, acho q...   \n",
       "1  sid6001  Acredito que todos possuem direito de escolher...   \n",
       "2   sid959  Acredito que todos tem o direito de escolher c...   \n",
       "3   sid842  Vejo que as pessoas tem todo o direito de esco...   \n",
       "4   sid774  As pessoas devem ser livres para tomar decisõe...   \n",
       "\n",
       "                                            freetext gender  ap.age  \\\n",
       "0  A flexibilidade é extremamente importante nas ...      m       1   \n",
       "1  Acabo de responder a perguntas bastante polêmi...      m       2   \n",
       "2  Achei esse teste cansativo à beça, mas me fez ...      m       2   \n",
       "3  amanhã trabalho cansaço tarde engenharia dados...      m       1   \n",
       "4  Algo difícil na vida é conquistar sua independ...      f       1   \n",
       "\n",
       "   num_palavras  \n",
       "0           272  \n",
       "1           416  \n",
       "2           373  \n",
       "3           664  \n",
       "4           304  "
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_merged.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "427.8137254901961"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_merged['num_palavras'].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3761\n",
      "3815\n",
      "3767\n",
      "3787\n",
      "3769\n",
      "3787\n",
      "3733\n",
      "3758\n",
      "3790\n",
      "3790\n"
     ]
    }
   ],
   "source": [
    "for i in range(1, 11):\n",
    "  train_df =  pd.read_csv(fold_path+r'\\k_'+str(i)+r'\\train.csv')\n",
    "  test_df =  pd.read_csv(fold_path+r'\\k_'+str(i)+r'\\test.csv')\n",
    "  train_df = train_df[['sid', 'concat', 'ap.age']]\n",
    "  test_df = test_df[['sid', 'concat', 'ap.age']]\n",
    "\n",
    "  train_df[\"Clean_text\"] = train_df.concat.apply(lambda x: clean_text(x))\n",
    "  train_df[\"Clean_text\"] = train_df.Clean_text.apply(lambda x: remove_stopwords(x))\n",
    "\n",
    "  test_df[\"Clean_text\"] = test_df.concat.apply(lambda x: clean_text(x))\n",
    "  test_df[\"Clean_text\"] = test_df.Clean_text.apply(lambda x: remove_stopwords(x))\n",
    "\n",
    "  unique_words = list(set(\" \".join(train_df.Clean_text.to_list()).split()))\n",
    "\n",
    "  threshold = train_df.shape[0]*0.01\n",
    "  word_counts = train_df['Clean_text'].str.split().explode().value_counts()\n",
    "  row_counts = pd.DataFrame({'word': word_counts.index, 'count': word_counts.values})\n",
    "  row_counts['count'] = row_counts['word'].apply(lambda word: train_df['Clean_text'].str.contains(word).sum())\n",
    "  selected_words = row_counts[row_counts['count']>threshold]['word'].to_list()\n",
    "  print(len(selected_words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def select_(col, perc): # mesma quantidade de palavras positivas e negativas\n",
    "  df_coef_1 = df_coef.copy()\n",
    "  df_coef_1 = df_coef_1[[col]]\n",
    "\n",
    "  str_col = str(col)\n",
    "\n",
    "  df_coef_1.rename(columns={col: str_col}, inplace=True)\n",
    "\n",
    "  df_coef_1_pos = df_coef_1[df_coef_1[str_col]>0]\n",
    "  df_coef_1_neg= df_coef_1[df_coef_1[str_col]<0]\n",
    "  df_coef_1_zero = df_coef_1[df_coef_1[str_col]==0]\n",
    "\n",
    "  df_coef_1_pos = df_coef_1_pos.sort_values(by=str_col, ascending=True)\n",
    "  df_coef_1_neg = df_coef_1_neg.sort_values(by=str_col, key=lambda x: abs(x))\n",
    "\n",
    "  pos_perc = df_coef_1_pos.head(int(len(df_coef_1_pos) * perc))\n",
    "  neg_perc = df_coef_1_neg.head(int(len(df_coef_1_neg) * perc))\n",
    "\n",
    "  indices_to_drop = pd.concat([pos_perc, neg_perc, df_coef_1_zero]).index\n",
    "\n",
    "  df_filtered = df_coef_1.drop(indices_to_drop)\n",
    "\n",
    "  return df_filtered.sort_values(by=str_col, ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df =  pd.read_csv(fold_path+r'\\k_'+str(1)+r'\\train.csv')\n",
    "test_df =  pd.read_csv(fold_path+r'\\k_'+str(1)+r'\\test.csv')\n",
    "train_df = train_df[['sid', 'concat', 'ap.age']]\n",
    "test_df = test_df[['sid', 'concat', 'ap.age']]\n",
    "\n",
    "train_df[\"Clean_text\"] = train_df.concat.apply(lambda x: clean_text(x))\n",
    "train_df[\"Clean_text\"] = train_df.Clean_text.apply(lambda x: remove_stopwords(x))\n",
    "\n",
    "test_df[\"Clean_text\"] = test_df.concat.apply(lambda x: clean_text(x))\n",
    "test_df[\"Clean_text\"] = test_df.Clean_text.apply(lambda x: remove_stopwords(x))\n",
    "\n",
    "unique_words = list(set(\" \".join(train_df.Clean_text.to_list()).split()))\n",
    "\n",
    "threshold = train_df.shape[0]*0.01\n",
    "word_counts = train_df['Clean_text'].str.split().explode().value_counts()\n",
    "row_counts = pd.DataFrame({'word': word_counts.index, 'count': word_counts.values})\n",
    "row_counts['count'] = row_counts['word'].apply(lambda word: train_df['Clean_text'].str.contains(word).sum())\n",
    "selected_words = row_counts[row_counts['count']>threshold]['word'].to_list()\n",
    "\n",
    "text = train_df.Clean_text.to_list()\n",
    "vectorizer = TfidfVectorizer(vocabulary=selected_words)\n",
    "train_matrix = vectorizer.fit_transform(text)\n",
    "train_matrix = train_matrix.toarray()\n",
    "vocab = vectorizer.get_feature_names_out()\n",
    "\n",
    "tf_idf = pd.DataFrame(data=train_matrix, columns=selected_words)\n",
    "\n",
    "X = tf_idf\n",
    "y =  train_df['ap.age']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>pessoas</th>\n",
       "      <th>acredito</th>\n",
       "      <th>deve</th>\n",
       "      <th>sociedade</th>\n",
       "      <th>ter</th>\n",
       "      <th>vida</th>\n",
       "      <th>deveria</th>\n",
       "      <th>forma</th>\n",
       "      <th>aborto</th>\n",
       "      <th>morte</th>\n",
       "      <th>...</th>\n",
       "      <th>manipula</th>\n",
       "      <th>ataca</th>\n",
       "      <th>sul</th>\n",
       "      <th>ocasiona</th>\n",
       "      <th>submete</th>\n",
       "      <th>tidos</th>\n",
       "      <th>festa</th>\n",
       "      <th>conhecer</th>\n",
       "      <th>sincera</th>\n",
       "      <th>mass</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.027904</td>\n",
       "      <td>0.081629</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.105703</td>\n",
       "      <td>0.033114</td>\n",
       "      <td>0.034667</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.072647</td>\n",
       "      <td>0.033647</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.062944</td>\n",
       "      <td>0.061379</td>\n",
       "      <td>0.026804</td>\n",
       "      <td>0.026494</td>\n",
       "      <td>0.049799</td>\n",
       "      <td>0.078202</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.081938</td>\n",
       "      <td>0.025300</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.239088</td>\n",
       "      <td>0.059663</td>\n",
       "      <td>0.088457</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.029011</td>\n",
       "      <td>0.094050</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.028157</td>\n",
       "      <td>0.080002</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.131206</td>\n",
       "      <td>0.023989</td>\n",
       "      <td>0.020952</td>\n",
       "      <td>0.062129</td>\n",
       "      <td>0.019463</td>\n",
       "      <td>0.061129</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.021350</td>\n",
       "      <td>0.059329</td>\n",
       "      <td>0.018730</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.049195</td>\n",
       "      <td>0.143916</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.062120</td>\n",
       "      <td>0.058382</td>\n",
       "      <td>0.061120</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.064040</td>\n",
       "      <td>0.029661</td>\n",
       "      <td>0.056182</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>454</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.153615</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.057080</td>\n",
       "      <td>0.089634</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.062611</td>\n",
       "      <td>0.028999</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>455</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.054648</td>\n",
       "      <td>0.047730</td>\n",
       "      <td>0.023588</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.046417</td>\n",
       "      <td>0.050159</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.045051</td>\n",
       "      <td>0.021334</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>456</th>\n",
       "      <td>0.113409</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.152258</td>\n",
       "      <td>0.036907</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.032379</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>457</th>\n",
       "      <td>0.090504</td>\n",
       "      <td>0.165475</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.028570</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.028110</td>\n",
       "      <td>0.151883</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.027283</td>\n",
       "      <td>0.051679</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>458</th>\n",
       "      <td>0.103396</td>\n",
       "      <td>0.120990</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.049082</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.055526</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>459 rows × 3761 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      pessoas  acredito      deve  sociedade       ter      vida   deveria  \\\n",
       "0    0.027904  0.081629  0.000000   0.105703  0.033114  0.034667  0.000000   \n",
       "1    0.062944  0.061379  0.026804   0.026494  0.049799  0.078202  0.000000   \n",
       "2    0.000000  0.239088  0.059663   0.088457  0.000000  0.029011  0.094050   \n",
       "3    0.131206  0.023989  0.020952   0.062129  0.019463  0.061129  0.000000   \n",
       "4    0.049195  0.143916  0.000000   0.062120  0.058382  0.061120  0.000000   \n",
       "..        ...       ...       ...        ...       ...       ...       ...   \n",
       "454  0.000000  0.000000  0.153615   0.000000  0.057080  0.089634  0.000000   \n",
       "455  0.000000  0.054648  0.047730   0.023588  0.000000  0.046417  0.050159   \n",
       "456  0.113409  0.000000  0.000000   0.000000  0.000000  0.000000  0.152258   \n",
       "457  0.090504  0.165475  0.000000   0.028570  0.000000  0.028110  0.151883   \n",
       "458  0.103396  0.120990  0.000000   0.000000  0.049082  0.000000  0.055526   \n",
       "\n",
       "        forma    aborto     morte  ...  manipula  ataca  sul  ocasiona  \\\n",
       "0    0.072647  0.033647  0.000000  ...       0.0    0.0  0.0       0.0   \n",
       "1    0.081938  0.025300  0.000000  ...       0.0    0.0  0.0       0.0   \n",
       "2    0.000000  0.028157  0.080002  ...       0.0    0.0  0.0       0.0   \n",
       "3    0.021350  0.059329  0.018730  ...       0.0    0.0  0.0       0.0   \n",
       "4    0.064040  0.029661  0.056182  ...       0.0    0.0  0.0       0.0   \n",
       "..        ...       ...       ...  ...       ...    ...  ...       ...   \n",
       "454  0.062611  0.028999  0.000000  ...       0.0    0.0  0.0       0.0   \n",
       "455  0.000000  0.045051  0.021334  ...       0.0    0.0  0.0       0.0   \n",
       "456  0.036907  0.000000  0.032379  ...       0.0    0.0  0.0       0.0   \n",
       "457  0.000000  0.027283  0.051679  ...       0.0    0.0  0.0       0.0   \n",
       "458  0.000000  0.000000  0.000000  ...       0.0    0.0  0.0       0.0   \n",
       "\n",
       "     submete  tidos  festa  conhecer  sincera  mass  \n",
       "0        0.0    0.0    0.0       0.0      0.0   0.0  \n",
       "1        0.0    0.0    0.0       0.0      0.0   0.0  \n",
       "2        0.0    0.0    0.0       0.0      0.0   0.0  \n",
       "3        0.0    0.0    0.0       0.0      0.0   0.0  \n",
       "4        0.0    0.0    0.0       0.0      0.0   0.0  \n",
       "..       ...    ...    ...       ...      ...   ...  \n",
       "454      0.0    0.0    0.0       0.0      0.0   0.0  \n",
       "455      0.0    0.0    0.0       0.0      0.0   0.0  \n",
       "456      0.0    0.0    0.0       0.0      0.0   0.0  \n",
       "457      0.0    0.0    0.0       0.0      0.0   0.0  \n",
       "458      0.0    0.0    0.0       0.0      0.0   0.0  \n",
       "\n",
       "[459 rows x 3761 columns]"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "4wAdWRM2oMN2",
    "outputId": "b213c83c-8535-4932-f558-2954e41ac476"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classificação por dicionário: 38 instâncias\n",
      "Porcentagem classificadas: 0.7450980392156863\n",
      "Classificadas corretamente: 0.5\n",
      "Classificação tradicional: 13 instâncias\n",
      "RESULTADO: 0.48081081081081084\n",
      "-----------------------------------------------------------------------\n",
      "Classificação por dicionário: 45 instâncias\n",
      "Porcentagem classificadas: 0.8823529411764706\n",
      "Classificadas corretamente: 0.5555555555555556\n",
      "Classificação tradicional: 6 instâncias\n",
      "RESULTADO: 0.5073610599926389\n",
      "-----------------------------------------------------------------------\n",
      "Classificação por dicionário: 41 instâncias\n",
      "Porcentagem classificadas: 0.803921568627451\n",
      "Classificadas corretamente: 0.6341463414634146\n",
      "Classificação tradicional: 10 instâncias\n",
      "RESULTADO: 0.5679671205986995\n",
      "-----------------------------------------------------------------------\n",
      "Classificação por dicionário: 38 instâncias\n",
      "Porcentagem classificadas: 0.7450980392156863\n",
      "Classificadas corretamente: 0.5263157894736842\n",
      "Classificação tradicional: 13 instâncias\n",
      "RESULTADO: 0.4593563815531779\n",
      "-----------------------------------------------------------------------\n",
      "Classificação por dicionário: 36 instâncias\n",
      "Porcentagem classificadas: 0.7058823529411765\n",
      "Classificadas corretamente: 0.5555555555555556\n",
      "Classificação tradicional: 15 instâncias\n",
      "RESULTADO: 0.46367521367521364\n",
      "-----------------------------------------------------------------------\n",
      "Classificação por dicionário: 40 instâncias\n",
      "Porcentagem classificadas: 0.7843137254901961\n",
      "Classificadas corretamente: 0.5\n",
      "Classificação tradicional: 11 instâncias\n",
      "RESULTADO: 0.407392253136934\n",
      "-----------------------------------------------------------------------\n",
      "Classificação por dicionário: 44 instâncias\n",
      "Porcentagem classificadas: 0.8627450980392157\n",
      "Classificadas corretamente: 0.4090909090909091\n",
      "Classificação tradicional: 7 instâncias\n",
      "RESULTADO: 0.37790697674418605\n",
      "-----------------------------------------------------------------------\n",
      "Classificação por dicionário: 41 instâncias\n",
      "Porcentagem classificadas: 0.803921568627451\n",
      "Classificadas corretamente: 0.4634146341463415\n",
      "Classificação tradicional: 10 instâncias\n",
      "RESULTADO: 0.3596296296296297\n",
      "-----------------------------------------------------------------------\n",
      "Classificação por dicionário: 41 instâncias\n",
      "Porcentagem classificadas: 0.803921568627451\n",
      "Classificadas corretamente: 0.6097560975609756\n",
      "Classificação tradicional: 10 instâncias\n",
      "RESULTADO: 0.5674723349141955\n",
      "-----------------------------------------------------------------------\n",
      "Classificação por dicionário: 42 instâncias\n",
      "Porcentagem classificadas: 0.8235294117647058\n",
      "Classificadas corretamente: 0.47619047619047616\n",
      "Classificação tradicional: 9 instâncias\n",
      "RESULTADO: 0.45198790978842124\n",
      "-----------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "for i in range(1, 11):\n",
    "  train_df =  pd.read_csv(fold_path+r'\\k_'+str(i)+r'\\train.csv')\n",
    "  test_df =  pd.read_csv(fold_path+r'\\k_'+str(i)+r'\\test.csv')\n",
    "  train_df = train_df[['sid', 'concat', 'ap.age']]\n",
    "  test_df = test_df[['sid', 'concat', 'ap.age']]\n",
    "\n",
    "  train_df[\"Clean_text\"] = train_df.concat.apply(lambda x: clean_text(x))\n",
    "  train_df[\"Clean_text\"] = train_df.Clean_text.apply(lambda x: remove_stopwords(x))\n",
    "\n",
    "  test_df[\"Clean_text\"] = test_df.concat.apply(lambda x: clean_text(x))\n",
    "  test_df[\"Clean_text\"] = test_df.Clean_text.apply(lambda x: remove_stopwords(x))\n",
    "\n",
    "  unique_words = list(set(\" \".join(train_df.Clean_text.to_list()).split()))\n",
    "\n",
    "  threshold = train_df.shape[0]*0.01\n",
    "  word_counts = train_df['Clean_text'].str.split().explode().value_counts()\n",
    "  row_counts = pd.DataFrame({'word': word_counts.index, 'count': word_counts.values})\n",
    "  row_counts['count'] = row_counts['word'].apply(lambda word: train_df['Clean_text'].str.contains(word).sum())\n",
    "  selected_words = row_counts[row_counts['count']>threshold]['word'].to_list()\n",
    "\n",
    "  text = train_df.Clean_text.to_list()\n",
    "  vectorizer = TfidfVectorizer(vocabulary=selected_words)\n",
    "  train_matrix = vectorizer.fit_transform(text)\n",
    "  train_matrix = train_matrix.toarray()\n",
    "  vocab = vectorizer.get_feature_names_out()\n",
    "\n",
    "  tf_idf = pd.DataFrame(data=train_matrix, columns=selected_words)\n",
    "\n",
    "  X = tf_idf\n",
    "  y =  train_df['ap.age']\n",
    "\n",
    "  svc = SVC(kernel='linear', decision_function_shape='ovr')\n",
    "  svc.fit(X, y)\n",
    "\n",
    "  df_coef = pd.DataFrame(data=svc.coef_, columns=tf_idf.columns)\n",
    "  df_coef = df_coef.T\n",
    "\n",
    "  coef1 = select_abs(0, 1)\n",
    "  coef2 = select_abs(1, 1)\n",
    "  coef3 = select_abs(2, 1)\n",
    "\n",
    "  train_df['glex1']=0\n",
    "  train_df['glex2']=0\n",
    "  train_df['glex3']=0\n",
    "\n",
    "  train_df.reset_index(inplace=True)\n",
    "\n",
    "  for index, row in train_df.iterrows():\n",
    "      sum = 0\n",
    "      text = row[\"Clean_text\"].split()\n",
    "      for w in text:\n",
    "        if w in coef1.index.tolist():\n",
    "          sum += coef1[0][w]*tf_idf[w][index]\n",
    "      sum = sum + svc.intercept_[0]\n",
    "      train_df[f'glex1'][index] = sum\n",
    "\n",
    "  for index, row in train_df.iterrows():\n",
    "      sum = 0\n",
    "      text = row[\"Clean_text\"].split()\n",
    "      for w in text:\n",
    "        if w in coef2.index.tolist():\n",
    "          sum += coef2[1][w]*tf_idf[w][index]\n",
    "      sum = sum + svc.intercept_[1]\n",
    "      train_df[f'glex2'][index] = sum\n",
    "\n",
    "  for index, row in train_df.iterrows():\n",
    "      sum = 0\n",
    "      text = row[\"Clean_text\"].split()\n",
    "      for w in text:\n",
    "        if w in coef3.index.tolist():\n",
    "          sum += coef3[2][w]*tf_idf[w][index]\n",
    "      sum = sum + svc.intercept_[2]\n",
    "      train_df[f'glex3'][index] = sum\n",
    "\n",
    "  train_df['c'] = 0\n",
    "\n",
    "  for index, row in train_df.iterrows():\n",
    "    if row['glex1'] > 0 and row['glex2'] > 0 and row['glex3'] > 0:\n",
    "      train_df['c'][index] = 1\n",
    "    elif row['glex1'] < 0 and row['glex2'] > 0 and row['glex3'] > 0:\n",
    "      train_df['c'][index] = 2\n",
    "    elif row['glex1'] < 0 and row['glex2'] < 0 and row['glex3'] < 0:\n",
    "      train_df['c'][index] = 3\n",
    "\n",
    "  train_df['ap.age'] = train_df['ap.age'].astype('int64')\n",
    "\n",
    "  # print(f'Porcentagem classificadas: {train_df[train_df.c!=0].shape[0]/train_df.shape[0]}')\n",
    "  # print(f'Porcentagem classificadas corretamente: {train_df[(train_df.c!=0) & (train_df.c==train_df[\"ap.age\"])].shape[0]/train_df[train_df.c!=0].shape[0]}')\n",
    "  # print(f'Porcentagem classificadas corretamente das totais: {train_df[(train_df.c!=0) & (train_df.c==train_df[\"ap.age\"])].shape[0]/train_df.shape[0]}')\n",
    "\n",
    "  test_matrix = vectorizer.transform(test_df[\"Clean_text\"].to_list())\n",
    "  test_matrix = test_matrix.toarray()\n",
    "\n",
    "  tf_idf_test = pd.DataFrame(data=test_matrix, columns=vocab)\n",
    "\n",
    "  test_df.reset_index(inplace=True)\n",
    "\n",
    "  test_df['glex1']=0\n",
    "  test_df['glex2']=0\n",
    "  test_df['glex3']=0\n",
    "\n",
    "  for index, row in test_df.iterrows():\n",
    "      sum = 0\n",
    "      text = row[\"Clean_text\"].split()\n",
    "      for w in text:\n",
    "        if w in coef1.index.tolist():\n",
    "          sum += coef1[0][w]*tf_idf_test[w][index]\n",
    "      sum = sum + svc.intercept_[0]\n",
    "      test_df[f'glex1'][index] = sum\n",
    "\n",
    "  for index, row in test_df.iterrows():\n",
    "      sum = 0\n",
    "      text = row[\"Clean_text\"].split()\n",
    "      for w in text:\n",
    "        if w in coef2.index.tolist():\n",
    "          sum += coef2[1][w]*tf_idf_test[w][index]\n",
    "      sum = sum + svc.intercept_[1]\n",
    "      test_df[f'glex2'][index] = sum\n",
    "\n",
    "  for index, row in test_df.iterrows():\n",
    "      sum = 0\n",
    "      text = row[\"Clean_text\"].split()\n",
    "      for w in text:\n",
    "        if w in coef3.index.tolist():\n",
    "          sum += coef3[2][w]*tf_idf_test[w][index]\n",
    "      sum = sum + svc.intercept_[2]\n",
    "      test_df[f'glex3'][index] = sum\n",
    "\n",
    "  test_df['c'] = 0\n",
    "\n",
    "  for index, row in test_df.iterrows():\n",
    "    if row['glex1'] > 0 and row['glex2'] > 0 and row['glex3'] > 0:\n",
    "      test_df['c'][index] = 1\n",
    "    elif row['glex1'] < 0 and row['glex2'] > 0 and row['glex3'] > 0:\n",
    "      test_df['c'][index] = 2\n",
    "    elif row['glex1'] < 0 and row['glex2'] < 0 and row['glex3'] < 0:\n",
    "      test_df['c'][index] = 3\n",
    "\n",
    "  test_df['ap.age'] = test_df['ap.age'].astype('int64')\n",
    "\n",
    "  print(f\"Classificação por dicionário: {test_df[test_df.c!=0].shape[0]} instâncias\")\n",
    "  print(f'Porcentagem classificadas: {test_df[test_df.c!=0].shape[0]/test_df.shape[0]}')\n",
    "  print(f'Classificadas corretamente: {test_df[(test_df.c!=0) & (test_df.c==test_df[\"ap.age\"])].shape[0]/test_df[test_df.c!=0].shape[0]}')\n",
    "  \n",
    "  print(f\"Classificação tradicional: {test_df[test_df.c==0].shape[0]} instâncias\")\n",
    "  \n",
    "  test_df_1 =  test_df[test_df.c==0]\n",
    "\n",
    "  tfvec = TfidfVectorizer(max_features = 3000)\n",
    "  tfvec.fit(train_df['Clean_text'].to_list())\n",
    "  tdf_train = tfvec.transform(train_df['Clean_text'].to_list()).toarray().tolist()\n",
    "  tdf_test = tfvec.transform(test_df_1['Clean_text'].to_list()).toarray().tolist()\n",
    "\n",
    "  age_train = train_df[\"ap.age\"].to_list()\n",
    "  age_test = test_df_1[\"ap.age\"].to_list()\n",
    "\n",
    "  logisticRegr = LogisticRegression(penalty=age_param[\"k_\"+str(i)][\"params\"][\"penalty\"], C=age_param[\"k_\"+str(i)][\"params\"][\"C\"], solver='liblinear')\n",
    "  logisticRegr.fit(tdf_train, age_train)\n",
    "  pred_age=logisticRegr.predict(tdf_test)\n",
    "  #print(f\"F1: {metrics.f1_score(age_test, pred_age, average='macro')}\")\n",
    "  \n",
    "  dict_class = test_df[test_df.c!=0]['ap.age']\n",
    "  dict_pred = test_df[test_df.c!=0]['c']\n",
    "\n",
    "  class_total = age_test+dict_class.to_list()\n",
    "  pred_total = pred_age.tolist()+dict_pred.to_list()\n",
    "\n",
    "  print(f\"RESULTADO: {metrics.f1_score(class_total, pred_total, average='macro')}\")\n",
    "  print(\"-----------------------------------------------------------------------\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(r'C:\\Users\\alici\\Documents\\UFLA\\mestrado\\projeto\\dicionario\\2024\\param_289_1.json', encoding='utf-8') as f:\n",
    "    dict_param = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'grid_values': {'C': [0.1, 1, 100],\n",
       "  'tol': [0.0001, 0.01],\n",
       "  'max_iter': [1000, -1]},\n",
       " 'k_2': {'params': {'C': 1, 'max_iter': 1000, 'tol': 0.0001},\n",
       "  'accuracy': 0.4942493521828578},\n",
       " 'k_3': {'params': {'C': 1, 'max_iter': 1000, 'tol': 0.0001},\n",
       "  'accuracy': 0.5113877055193046},\n",
       " 'k_4': {'params': {'C': 100, 'max_iter': 1000, 'tol': 0.0001},\n",
       "  'accuracy': 0.46401666920941836},\n",
       " 'k_5': {'params': {'C': 100, 'max_iter': 1000, 'tol': 0.0001},\n",
       "  'accuracy': 0.4848739398198131},\n",
       " 'k_6': {'params': {'C': 1, 'max_iter': 1000, 'tol': 0.0001},\n",
       "  'accuracy': 0.49088131030976634},\n",
       " 'k_7': {'params': {'C': 1, 'max_iter': 1000, 'tol': 0.0001},\n",
       "  'accuracy': 0.48728239876892576},\n",
       " 'k_8': {'params': {'C': 100, 'max_iter': 1000, 'tol': 0.0001},\n",
       "  'accuracy': 0.4764956996401838},\n",
       " 'k_9': {'params': {'C': 100, 'max_iter': 1000, 'tol': 0.0001},\n",
       "  'accuracy': 0.4838233617005646},\n",
       " 'k_10': {'params': {'C': 100, 'max_iter': 1000, 'tol': 0.0001},\n",
       "  'accuracy': 0.4856383821026232},\n",
       " 'k_11': {'params': {'C': 100, 'max_iter': 1000, 'tol': 0.0001},\n",
       "  'accuracy': 0.4866555693537471},\n",
       " 'time': '5.0min2.19s'}"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dict_param"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1 (trad-dict): 0.4738134964775677\n",
      "F1 (dict): 0.5107239376689092\n",
      "RESULTADO: 0.48081081081081084\n",
      "-----------------------------------------------------------------------\n",
      "F1 (trad-dict): 0.5223706229673835\n",
      "F1 (dict): 0.5186495650891936\n",
      "RESULTADO: 0.5073610599926389\n",
      "-----------------------------------------------------------------------\n",
      "F1 (trad-dict): 0.491027926322044\n",
      "F1 (dict): 0.565826330532213\n",
      "RESULTADO: 0.5679671205986995\n",
      "-----------------------------------------------------------------------\n",
      "F1 (trad-dict): 0.40803621958121106\n",
      "F1 (dict): 0.3924963924963925\n",
      "RESULTADO: 0.4593563815531779\n",
      "-----------------------------------------------------------------------\n",
      "F1 (trad-dict): 0.451739618406285\n",
      "F1 (dict): 0.4791666666666667\n",
      "RESULTADO: 0.4626180836707152\n",
      "-----------------------------------------------------------------------\n",
      "F1 (trad-dict): 0.1904761904761905\n",
      "F1 (dict): 0.46969696969696967\n",
      "RESULTADO: 0.407392253136934\n",
      "-----------------------------------------------------------------------\n",
      "F1 (trad-dict): 0.3781986531986532\n",
      "F1 (dict): 0.3548030916451969\n",
      "RESULTADO: 0.37790697674418605\n",
      "-----------------------------------------------------------------------\n",
      "F1 (trad-dict): 0.1954022988505747\n",
      "F1 (dict): 0.470940170940171\n",
      "RESULTADO: 0.3596296296296297\n",
      "-----------------------------------------------------------------------\n",
      "F1 (trad-dict): 0.37777777777777777\n",
      "F1 (dict): 0.4589562024652505\n",
      "RESULTADO: 0.5674723349141955\n",
      "-----------------------------------------------------------------------\n",
      "F1 (trad-dict): 0.3175243215565796\n",
      "F1 (dict): 0.45635673624288436\n",
      "RESULTADO: 0.45198790978842124\n",
      "-----------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "for i in range(1, 11):\n",
    "  train_df =  pd.read_csv(fold_path+r'\\k_'+str(i)+r'\\train.csv')\n",
    "  test_df =  pd.read_csv(fold_path+r'\\k_'+str(i)+r'\\test.csv')\n",
    "  train_df = train_df[['sid', 'concat', 'ap.age']]\n",
    "  test_df = test_df[['sid', 'concat', 'ap.age']]\n",
    "\n",
    "  train_df[\"Clean_text\"] = train_df.concat.apply(lambda x: clean_text(x))\n",
    "  train_df[\"Clean_text\"] = train_df.Clean_text.apply(lambda x: remove_stopwords(x))\n",
    "\n",
    "  test_df[\"Clean_text\"] = test_df.concat.apply(lambda x: clean_text(x))\n",
    "  test_df[\"Clean_text\"] = test_df.Clean_text.apply(lambda x: remove_stopwords(x))\n",
    "\n",
    "  unique_words = list(set(\" \".join(train_df.Clean_text.to_list()).split()))\n",
    "\n",
    "  threshold = train_df.shape[0]*0.01\n",
    "  word_counts = train_df['Clean_text'].str.split().explode().value_counts()\n",
    "  row_counts = pd.DataFrame({'word': word_counts.index, 'count': word_counts.values})\n",
    "  row_counts['count'] = row_counts['word'].apply(lambda word: train_df['Clean_text'].str.contains(word).sum())\n",
    "  selected_words = row_counts[row_counts['count']>threshold]['word'].to_list()\n",
    "\n",
    "  text = train_df.Clean_text.to_list()\n",
    "  vectorizer = TfidfVectorizer(vocabulary=selected_words)\n",
    "  train_matrix = vectorizer.fit_transform(text)\n",
    "  train_matrix = train_matrix.toarray()\n",
    "  vocab = vectorizer.get_feature_names_out()\n",
    "\n",
    "  tf_idf = pd.DataFrame(data=train_matrix, columns=selected_words)\n",
    "\n",
    "  X = tf_idf\n",
    "  y =  train_df['ap.age']\n",
    "\n",
    "  svc = SVC(kernel='linear', decision_function_shape='ovr')\n",
    "  svc.fit(X, y)\n",
    "\n",
    "  df_coef = pd.DataFrame(data=svc.coef_, columns=tf_idf.columns)\n",
    "  df_coef = df_coef.T\n",
    "\n",
    "  coef1 = select_abs(0, 1)\n",
    "  coef2 = select_abs(1, 1)\n",
    "  coef3 = select_abs(2, 1)\n",
    "\n",
    "  train_df['glex1']=0\n",
    "  train_df['glex2']=0\n",
    "  train_df['glex3']=0\n",
    "\n",
    "  train_df.reset_index(inplace=True)\n",
    "\n",
    "  for index, row in train_df.iterrows():\n",
    "      sum = 0\n",
    "      text = row[\"Clean_text\"].split()\n",
    "      for w in text:\n",
    "        if w in coef1.index.tolist():\n",
    "          sum += coef1[0][w]*tf_idf[w][index]\n",
    "      sum = sum + svc.intercept_[0]\n",
    "      train_df[f'glex1'][index] = sum\n",
    "\n",
    "  for index, row in train_df.iterrows():\n",
    "      sum = 0\n",
    "      text = row[\"Clean_text\"].split()\n",
    "      for w in text:\n",
    "        if w in coef2.index.tolist():\n",
    "          sum += coef2[1][w]*tf_idf[w][index]\n",
    "      sum = sum + svc.intercept_[1]\n",
    "      train_df[f'glex2'][index] = sum\n",
    "\n",
    "  for index, row in train_df.iterrows():\n",
    "      sum = 0\n",
    "      text = row[\"Clean_text\"].split()\n",
    "      for w in text:\n",
    "        if w in coef3.index.tolist():\n",
    "          sum += coef3[2][w]*tf_idf[w][index]\n",
    "      sum = sum + svc.intercept_[2]\n",
    "      train_df[f'glex3'][index] = sum\n",
    "\n",
    "  train_df['c'] = 0\n",
    "\n",
    "  for index, row in train_df.iterrows():\n",
    "    if row['glex1'] > 0 and row['glex2'] > 0 and row['glex3'] > 0:\n",
    "      train_df['c'][index] = 1\n",
    "    elif row['glex1'] < 0 and row['glex2'] > 0 and row['glex3'] > 0:\n",
    "      train_df['c'][index] = 2\n",
    "    elif row['glex1'] < 0 and row['glex2'] < 0 and row['glex3'] < 0:\n",
    "      train_df['c'][index] = 3\n",
    "\n",
    "  train_df['ap.age'] = train_df['ap.age'].astype('int64')\n",
    "\n",
    "  # print(f'Porcentagem classificadas: {train_df[train_df.c!=0].shape[0]/train_df.shape[0]}')\n",
    "  # print(f'Porcentagem classificadas corretamente: {train_df[(train_df.c!=0) & (train_df.c==train_df[\"ap.age\"])].shape[0]/train_df[train_df.c!=0].shape[0]}')\n",
    "  # print(f'Porcentagem classificadas corretamente das totais: {train_df[(train_df.c!=0) & (train_df.c==train_df[\"ap.age\"])].shape[0]/train_df.shape[0]}')\n",
    "\n",
    "  test_matrix = vectorizer.transform(test_df[\"Clean_text\"].to_list())\n",
    "  test_matrix = test_matrix.toarray()\n",
    "\n",
    "  tf_idf_test = pd.DataFrame(data=test_matrix, columns=vocab)\n",
    "\n",
    "  test_df.reset_index(inplace=True)\n",
    "\n",
    "  test_df['glex1']=0\n",
    "  test_df['glex2']=0\n",
    "  test_df['glex3']=0\n",
    "\n",
    "  for index, row in test_df.iterrows():\n",
    "      sum = 0\n",
    "      text = row[\"Clean_text\"].split()\n",
    "      for w in text:\n",
    "        if w in coef1.index.tolist():\n",
    "          sum += coef1[0][w]*tf_idf_test[w][index]\n",
    "      sum = sum + svc.intercept_[0]\n",
    "      test_df[f'glex1'][index] = sum\n",
    "\n",
    "  for index, row in test_df.iterrows():\n",
    "      sum = 0\n",
    "      text = row[\"Clean_text\"].split()\n",
    "      for w in text:\n",
    "        if w in coef2.index.tolist():\n",
    "          sum += coef2[1][w]*tf_idf_test[w][index]\n",
    "      sum = sum + svc.intercept_[1]\n",
    "      test_df[f'glex2'][index] = sum\n",
    "\n",
    "  for index, row in test_df.iterrows():\n",
    "      sum = 0\n",
    "      text = row[\"Clean_text\"].split()\n",
    "      for w in text:\n",
    "        if w in coef3.index.tolist():\n",
    "          sum += coef3[2][w]*tf_idf_test[w][index]\n",
    "      sum = sum + svc.intercept_[2]\n",
    "      test_df[f'glex3'][index] = sum\n",
    "\n",
    "  test_df['c'] = 0\n",
    "\n",
    "  for index, row in test_df.iterrows():\n",
    "    if row['glex1'] > 0 and row['glex2'] > 0 and row['glex3'] > 0:\n",
    "      test_df['c'][index] = 1\n",
    "    elif row['glex1'] < 0 and row['glex2'] > 0 and row['glex3'] > 0:\n",
    "      test_df['c'][index] = 2\n",
    "    elif row['glex1'] < 0 and row['glex2'] < 0 and row['glex3'] < 0:\n",
    "      test_df['c'][index] = 3\n",
    "\n",
    "  test_df['ap.age'] = test_df['ap.age'].astype('int64')\n",
    "\n",
    "  #print(f\"Classificação por dicionário: {test_df[test_df.c!=0].shape[0]} instâncias\")\n",
    "  #print(f'Porcentagem classificadas: {test_df[test_df.c!=0].shape[0]/test_df.shape[0]}')\n",
    "  #print(f'Classificadas corretamente: {test_df[(test_df.c!=0) & (test_df.c==test_df[\"ap.age\"])].shape[0]/test_df[test_df.c!=0].shape[0]}')\n",
    "\n",
    "  #print(f\"Classificação tradicional: {test_df[test_df.c==0].shape[0]} instâncias\")\n",
    "\n",
    "  age_test_dict = test_df[test_df.c!=0][\"ap.age\"].to_list()\n",
    "  pred_age_dict = test_df[test_df.c!=0][\"c\"].to_list()\n",
    "\n",
    "  tfvec = TfidfVectorizer(max_features = 3000)\n",
    "  tfvec.fit(train_df['Clean_text'].to_list())\n",
    "  tdf_train = tfvec.transform(train_df['Clean_text'].to_list()).toarray().tolist()\n",
    "\n",
    "  age_train = train_df[\"ap.age\"].to_list()\n",
    "\n",
    "  logisticRegr = LogisticRegression(penalty=age_param[\"k_\"+str(i)][\"params\"][\"penalty\"], C=age_param[\"k_\"+str(i)][\"params\"][\"C\"], solver='liblinear')\n",
    "  logisticRegr.fit(tdf_train, age_train)\n",
    "  \n",
    "  test_df_dict = test_df[test_df.c!=0]\n",
    "  tdf_test_dict = tfvec.transform(test_df_dict['Clean_text'].to_list()).toarray().tolist()\n",
    "\n",
    "  pred_age_trad_dict=logisticRegr.predict(tdf_test_dict)\n",
    "\n",
    "  print(f\"F1 (trad-dict): {metrics.f1_score(age_test_dict, pred_age_trad_dict, average='macro')}\")\n",
    "  print(f\"F1 (dict): {metrics.f1_score(age_test_dict, pred_age_dict, average='macro')}\")\n",
    "\n",
    "  test_df_noclass_dict = test_df[test_df.c==0]\n",
    "  age_test_noclass_dict = test_df_noclass_dict[\"ap.age\"].to_list()\n",
    "  tdf_test_noclass_dict = tfvec.transform(test_df_noclass_dict['Clean_text'].to_list()).toarray().tolist()\n",
    "\n",
    "  pred_age_noclass_dict=logisticRegr.predict(tdf_test_noclass_dict) \n",
    "\n",
    "  class_total = age_test_dict+age_test_noclass_dict\n",
    "  pred_total = pred_age_dict+pred_age_noclass_dict.tolist()\n",
    "\n",
    "  print(f\"RESULTADO: {metrics.f1_score(class_total, pred_total, average='macro')}\")\n",
    "  print(\"-----------------------------------------------------------------------\")"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
